<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Projects</title>
    <style>
        footer {
            display: block;
            width: 100%;
            background-color: rgb(51, 58, 65);
            color: white;
            text-align: center;
        }
        .jumbotron {
            margin-top: 10px;
            background-image: url("img/projects2.jpg");
            background-size: cover;
        }
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
        }
        .rowimg {
            display: flex;
        }

        /* Create three equal columns that sits next to each other */
        .column2img {
            flex: 50%;
            padding: 5px;
        }

        .column3img {
            flex: 33.33%;
            padding: 5px;
        }
    </style>
  </head>
  <body>
    <div class="container-fluid">
        <!-- <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link" href="home.html">Home</a>
                        </li>
                        <li class="nav-item active">
                            <a class="nav-link" href="#">Projects <span class="sr-only">(current)</span></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="publication.html">Publications</a>
                        </li>
                        <li class="nav-item ">
                            <a class="nav-link" href="contact.html">Contact <span class="sr-only">(current)</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav> -->
        <div class="jumbotron jumbotron-fluid">
            <div class="container">
                <h1 class="display-3">Projects</h1>
                <p class="lead"></p>
            </div>
        </div>
        <div class="container">
            <!--
            <div>
                <h4><mark>Reduced Visual Interaction during Audio Skimming</mark></h4>
                <br/>
                <p>
                    This project is my Master's thesis at UBC. <br/>
                    <b>Problem:</b> Initially, the project started as an <mark>accessibility</mark> problem. I have an hour commute
                    to UBC from my home and often times I find myself in a difficult situation - I need to read/skim an article, but unfortunately, I have motion
                    sickness. How about skimming audio (text-to-speech) to compensate for this situational impairment? It might help, but we don't know.
                </p>
                
                <p>
                    <b>Empathize:</b> In the empathize stage, I looked into existing voice reading apps available in the iOS and Android app store. I noted down what's common and different in 
                    these apps. 
                    To get a deeper understanding of how audio skimming might work, I conducted a laboratory study with an existing voice reading app as a <mark>technological probe</mark>. In the study, I observed 
                    people skimming auditorily while I noted down interesting events.
                </p>
                <p>
                    <b>Define:</b> The user study provided insights on what the user's really cared about. Some of my initial assumptions turned out to be false. I drafted requirements and task examples to nail down 
                    the problem crisply.
                </p>
                <p>
                    <b>Ideate:</b> I started ideating over multiple design possibilities. I started designing for individual features, and then combined them to shape 
                    into a holistic mental model. Finding the right mental model was challenging. However, to be honest, exploring different design spaces was a fun exercise.  
                </p>
                <div class="text-center">
                    <a href="img/audioskimming.JPG" target="_blank">
                        <img src="img/audioskimming.JPG" width="400" alt="Paper prototyping" style="margin-top: 20px; margin-bottom: 30px">
                    </a>
                </div>
                <p>
                    <b>Prototype:</b> Prototyping is a fun activity. Initially I sketched ideas for inidividual features. Then I created a paper prototype. The overall design process
                    was iterative. I gathered feedback from my peers and supervisors. Currently I am exploring <mark>proto.io</mark> to create a medium fidelity prototype.
                </p>
            </div>
            -->
            <div>
                <h4><mark>Designing an Eyes-Reduced Document Skimming App for Situational Impairments</mark></h4>
                <br>
                <div class="text-center">
                    <iframe 
                        style="margin-bottom:15px"
                        width="560" height="315" 
                        src="https://youtube.com/embed/b62MvfLORmw" 
                        frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                    </iframe>
                </div>
                <p>
                    Check out for more details <a href="https://dl.acm.org/doi/pdf/10.1145/3313831.3376641" target="_blank">our paper</a> published in CHI 2020.
                </p>
            </div>
            <br>
            <br>
            <div>
                <h4><mark>EyePen and FLight: Reading Tool for People with Vision Impairment</mark></h4>
                <br/>
                <div class="text-center">
                    <iframe 
                        style="margin-bottom:15px"
                        width="560" height="315" 
                        src="https://www.youtube.com/embed/siu1-LsHnh0" 
                        frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                    </iframe>
                </div>
                <p>
                    EyePen and FLight are both results of 18 months ethnographic work at a local school for children with vision 
                    impairments. My contributions in this project were - study design and conducting ethnography, co-designing 
                    the tool, study design for evaluation, and data analysis.
                </p>
                <div class="text-center">
                    <div class="rowimg">
                        <div class="column2img">
                            <a href="img/book2.jpg" target="_blank">
                                <img src="img/book2.jpg" width="400" alt="Paper prototyping" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column2img">
                            <a href="img/mathtool.jpg" target="_blank">
                                <img src="img/mathtool.jpg" width="400" alt="Paper prototyping" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                    </div>
                </div>
                <p>
                    <b>Empathize:</b> In this 18 months long ethnography, we observed that children with vision impairments contain themselves within academic books 
                    only. They did not read story books simply because Braille printed books were either not available or simple too expensive. For mathamatics, they used 
                    a slate with ridges and holes, to insert lead bars in any prefixed orientation to mean a numerical number. We dug through the 
                    literature only to find solutions that are easily accessible to first world nations - such as interventions through laptops, tablets, or smartphones. 
                    However, we found less privileged children with vision impairments did not have access to these tools.
                </p>
                <p>
                    <b>Define:</b> We defined a set of requirements based on their needs. Among priority requirements were - low cost tools, easy to use, readily accessible, 
                    and readily replaceable upon replenishment. Our primary target was to reach a solution tools supporting reading and writing would be low cost. We knew that 
                    ink-based systems are cheaper than Braille embossed systems. However, ink is flat and provides no tactile or haptic feedback. 
                </p>
                <p>
                    <b>Ideate:</b> We ideated over couple of design directions. One direction was to build a low cost Braille printer, however, much research on Braille 
                    printers led to few albeit accessible Braille printers. The other direction that we ultimately pursued was to play with lights. A coupling of LED and 
                    phototransistor can distinguish a white surface from a black colored surface. 
                </p>
                <div class="text-center">
                    <div class="rowimg">
                        <div class="column2img">
                            <a href="img/flightreading.png" target="_blank">
                                <img src="img/flightreading.png" width="400" alt="Paper prototyping" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column2img">
                            <a href="img/flightwriting.png" target="_blank">
                                <img src="img/flightwriting.png" width="400" alt="Paper prototyping" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                    </div>
                </div>
                <p>
                    <b>Prototype:</b> We developed a testbed system and tested with a focus group consisting of 
                    both visually impared students and their teachers. In a participatory design study, we thrived to improve our prototype in two phases. The first phase we called 
                    it EyePen, helped children to read by providing audio pulses when there is a Braille dot. In the second phase, we improved the design to a wearable glove, using 
                    a simple coupling of six LEDs to six Braille dots with a single phototransistor, providing alphabetical output instead of audio pulses. We called it FLight. For both 
                    of these phases, we used a trajectory board to help the reader orient the tool while reading. 
                </p>
                <p>
                    <b>Test:</b> We asked our focus group members to read using our tool. We received interesting feedback from them, as well as design implications for future. 
                    We also came to learn other interesting problems that children with visual impairment face while reading. For example, cognitive load is often high while reading 
                    because they have to remember individual letters before they form a word. 
                </p>
                <p>We published this work in <a href="https://dl.acm.org/citation.cfm?id=3001915" target="_blank">ACM DEV 2016</a> and <a href="https://dl.acm.org/citation.cfm?id=3025646" target="_blank">ACM CHI 2017</a>. </p>
            </div>
            <br/>
            <br/>
            <div>
                <h4><mark>Speech-Based Source Code Comprehension Tool for Developers</mark></h4>
                <br/>
                <p>
                    This project was part of the CPSC 507 Software Engineering course. My team members were Shareen Mahmud and Salvador Corts. Laura Cang was kind enough to 
                    add voice to the comments in this demo. 
                </p>
                <p>
                    <b>Problem:</b> Developers spend a significant amount of time on program comprehension. Source code authors usually add text-based comments 
                    to maintain a documentation of their artifact, as well as to help others understand their source code. However, text-based comments often lack the necessary depth 
                    and expressiveness to communicate interpretations of complex concepts. As a result, developers often resort to face-to-face communication. This can particularly be 
                    challenging for newcomers on-boarding a project where co-workers may be unavailable.
                </p>
                <p>
                    <b>Empathize:</b> We conducted a literature survey to understand the breadth and depth of speech-based commenting in the literature. While we found few studies 
                    addressing speech-based comments, we identified that the literature did not have conceptual clarity on the nuances of speech-based comments - for example, duration
                    of speech comments, who are the users, the pros and cons of speech-based and text-based comments.
                </p>
                <div class="text-center">
                    <a href="img/ide1.png" target="_blank">
                        <img src="img/ide1.png" width="500" alt="A developer tool with speech comments" style="margin-top: 10px; margin-bottom: 25px">
                    </a>
                </div>
                <p>
                    <b>Define:</b> Our study contributed to the conceptual gap in understanding nature and substances of speech-based comments in source code comprehension. We implemented 
                    a high-fidelity prototype IDE in Java. Our prototype was a simplified version of Eclipse. <br/>
                    My contributions in tool development were - developing speech comment recording module, integrating IBM's Watson Speech to text module to generate transcripts, and synchronise
                    audio and visual play heads.
                </p>
                <div class="text-center">
                    <a href="img/userstudy.jpg" target="_blank">
                        <img src="img/userstudy.jpg" width="500" alt="A participant using out tool with speech-based comments" style="margin-top: 10px; margin-bottom: 25px">
                    </a>
                </div>
                <p>
                    Next, we conducted a controlled laboratory study to understand how programmers interact with speech-based comments. We recruited 10 participants 
                    having varied levels of expertise on Java. We designed a <mark>2x2 within-subjects</mark> study. Participants saw two different source codes - merge 
                    sort and quick sort, authored and annotated by an expert programmer working in Google. Each participants saw one of the codes with speech comments and the other 
                    in text comments. We altered method and variable names. The task was to comprehend the source code with the help of the comments. We tested for comprehension 
                    by conducting a post-task questionnaire and a semi-structured interview.
                </p>
                <p>
                    <b>Findings:</b> Some of the key findings were - duration of speech comments 
                    should be within a minute and transcripts of speech comments should always remain available. Moreover, speech comments were more helpful to new hires while 
                    onboarding a new codebase. Besides, participants rated speech comments to be most helpful when it describes the code flow and how multiple files
                    are working together to produce that flow.
                </p>
            </div>
            <br/>
            <br/>
            <div>
                <h4><mark>Personalised and Augmented Web History</mark></h4>
                <br/>
                <div class="text-center">
                    <iframe 
                        style="margin-top: 10px; margin-bottom: 20px" width="560" height="315" src="https://www.youtube.com/embed/7oNMHKcg0vY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                    </iframe>
                </div>
                <p>
                    Re-finding previously visited information on the web has been a long-standing problem. Existing re-finding solutions rely on users manually inputting 
                    user-defined tags or browsing through a laundry list of web pages in History. I investigated this problem with Shareen Mahmud, Borke Obada, and Amelia Cole. 
                </p>
                <p>
                    <b>Empathize:</b> In the empathize stage, we surveyed literature on information searching behaviors. We also conducted a semi-structured observation
                    to understand how people search for information on the web and what strategies they use. We recruited 10 participants and gave them a search task, then we 
                    interrupted them in the middle of that task, and asked them to re-find one of the previously found pages again. We also conducted an online survey and 
                    received 71 responses. 
                </p>
                <p>
                    <b>Define:</b> Empathizing with the users helped us to learn that - (1) people need support in the form of cues to re-find information, and (2) people 
                    need tool support during their wayfinding behavior in re-finding episodes. Therefore, our primary design direction 
                    was to redesign Mozilla's web browsing history in order to support a rich set of contextual cues. Informed by the literature and the data from observations, 
                    interviews, and questionnaires we identified the following contextual cues to aid information re-finding:
                    <ol>
                        <li>Person: who you shared it with</li>
                        <li>Website: where you saw it</li>
                        <li>File: what type the file was</li>
                        <li>Date: when you accessed it</li>
                        <li>Activity: what you did with it</li>
                        <li>Device: which device you saw it on</li>
                    </ol>
                    We chose to integrate the search history result with the contextual cue filters within the search engine result page. This was due to the fact that searching 
                    on Google was the most preferred re-finding method for people and they often neglected consulting the web history page.
                </p>
                <div class="text-center">
                    <div class="rowimg">
                        <div class="column3img">
                            <a href="img/storyboard1.png" target="_blank">
                                <img src="img/storyboard1.png" width="300" alt="Storyboarding PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column3img">
                            <a href="img/storyboard2.png" target="_blank">
                                <img src="img/storyboard2.png" width="300" alt="Storyboarding PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column3img">
                            <a href="img/storyboard3.png" target="_blank">
                                <img src="img/storyboard3.png" width="300" alt="Storyboarding PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                    </div>
                </div>
                <p>
                    <b>Ideate:</b> In this stage, we created task descriptions, requirements, conceptual design, and storyboards. For task descriptions, we proposed 
                    two different re-finding episodes - (1) looking for a specific web page, and (2) looking for any instance of an information. We identified functional 
                    and non-functional requirements. It took few iterations to make peace with conceptual design.
                </p>
                <p>
                    <b>Prototype:</b> We iterated over two different design concepts. We created paper prototype of these design concepts and evaluated them by running 
                    Cognitive Walkthrough with some tech-savvy people. We received both positive and negative feedbacks on our design. We refined our design and created
                    a medium fidelity prototype using <mark>Justinmind</mark>. 
                </p>
                <div class="text-center">
                    <div class="rowimg">
                        <div class="column3img">
                            <a href="img/recipeflow21.jpg" target="_blank">
                                <img src="img/recipeflow21.jpg" width="300" alt="Paper prototyping of PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column3img">
                            <a href="img/recipeflow22.jpg" target="_blank">
                                <img src="img/recipeflow22.jpg" width="300" alt="Paper prototyping of PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column3img">
                            <a href="img/recipeflow23.jpg" target="_blank">
                                <img src="img/recipeflow23.jpg" width="300" alt="Paper prototyping of PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                    </div>
                    <div class="rowimg">
                        <div class="column3img">
                            <a href="img/recipeflow24.jpg" target="_blank">
                                <img src="img/recipe flow 2_4.jpg" width="300" alt="Paper prototyping of PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column3img">
                            <a href="img/recipeflow25.jpg" target="_blank">
                                <img src="img/recipeflow25.jpg" width="300" alt="Paper prototyping of PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                        <div class="column3img">
                            <a href="img/recipeflow26.jpg" target="_blank">
                                <img src="img/recipeflow26.jpg" width="300" alt="Paper prototyping of PAWH" style="margin-top: 20px; margin-bottom: 30px">
                            </a>
                        </div>
                    </div>
                </div>
                <p>
                    <b>Test:</b> The challenge of testing re-finding episodes is that it is difficult to generate a re-finding episode that people actually care about.
                    When we finally tested our prototype, we wanted to test the usefulness of cues and usability of our design. We compared our tool against the more 
                    popular tool - Google search. We created two isomorphic tasks - finding a recipe seen before, and buying a Voice Assistant seen before. We recruited 
                    8 participants. We observed their usage behavior, use of cues, page visits, and clicks. We also calculated user engagement by using a survey. Our results 
                    indicated that re-finding was more successful with our tool than Google (100% vs. 38% success rates). Some people reported being confused by the 
                    use of contextual cues. However, all participants reported to have found our tool useful.
                </p>
            </div>
            <br/>
            <br/>
        </div>
    </div>
    <footer>
        <p><small>Taslim Arefin Khan | 2019</small></p>
    </footer>
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>
